{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3316\\2882034873.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_set['Discussion'] = lst\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.3875 - loss: 1.3612 - val_accuracy: 0.6475 - val_loss: 0.9115\n",
      "Epoch 2/5\n",
      "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.7199 - loss: 0.7705 - val_accuracy: 0.6744 - val_loss: 0.8682\n",
      "Epoch 3/5\n",
      "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 23ms/step - accuracy: 0.7846 - loss: 0.6032 - val_accuracy: 0.6667 - val_loss: 0.9457\n",
      "Epoch 4/5\n",
      "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.8323 - loss: 0.4582 - val_accuracy: 0.6513 - val_loss: 1.1082\n",
      "Epoch 5/5\n",
      "\u001b[1m617/617\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.8729 - loss: 0.3472 - val_accuracy: 0.6434 - val_loss: 1.3571\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6481 - loss: 1.3546\n",
      "Test Accuracy: 0.6434\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Dropout, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "%run general_preprocessing.ipynb\n",
    "# Assuming your dataset has been processed\n",
    "# data_set = process_data(data_set)  # As you already did this\n",
    "data_set = pd.read_csv('F:/MY_Projects/Deep_Learning_project/data/train.csv')\n",
    "label_mapping = {\n",
    "    \"Politics\": 0,\n",
    "    \"Sports\": 1,\n",
    "    \"Media\": 2,\n",
    "    \"Market & Economy\": 3,\n",
    "    \"STEM\": 4\n",
    "}\n",
    "\n",
    "# Apply the mapping to the label column\n",
    "data_set['Category'] = data_set['Category'].map(label_mapping)\n",
    "data_set = process_data(data_set)\n",
    "\n",
    "X = data_set['Discussion']\n",
    "y = data_set['Category']\n",
    "\n",
    "# Step 1: Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=5000)  # Set the vocabulary size to 5000 words\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Step 3: Pad the sequences to ensure uniform length\n",
    "max_len = 100  # You can adjust this based on the average length of your text\n",
    "X_train_pad = pad_sequences(X_train_seq, padding='post', maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=max_len)\n",
    "\n",
    "# Step 4: Build the TextCNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding Layer: Converts words into word vectors\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_len))\n",
    "\n",
    "# Convolutional Layer: Filters and captures local dependencies in the text\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "\n",
    "# MaxPooling Layer: Reduces the dimensionality by taking the max value over a pool of words\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Add a second convolutional layer\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Global MaxPooling: Extracts the most important feature from the entire sequence\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# Dense Layer: Fully connected layer for classification\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Dropout to prevent overfitting\n",
    "model.add(Dense(5, activation='softmax'))  # 5 categories\n",
    "\n",
    "# Step 5: Compile the model\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save_as_numbers(model, tokenizer, input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Predict categories for discussions in a CSV file and save SampleID with numeric labels.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained TextCNN model.\n",
    "    - tokenizer: Tokenizer used during training.\n",
    "    - input_csv: Path to the input CSV file containing 'SampleID' and 'Discussion' columns.\n",
    "    - output_csv: Path to save the output CSV file with SampleID and predicted numeric labels.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    # Step 1: Load the input CSV file\n",
    "    input_data = pd.read_csv(input_csv)\n",
    "\n",
    "    # Step 2: Preprocess the Discussion column\n",
    "    sample_ids = input_data['SampleID']  # Extract SampleID for output\n",
    "    input_data = process_data(input_data)\n",
    "    discussions = input_data['Discussion'].values\n",
    "    sequences = tokenizer.texts_to_sequences(discussions)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "    # Step 3: Predict labels using the model\n",
    "    predictions = model.predict(padded_sequences)\n",
    "    predicted_labels = predictions.argmax(axis=1)  # Get numeric labels\n",
    "\n",
    "    # Step 4: Create a new DataFrame with SampleID and predicted labels\n",
    "    output_data = pd.DataFrame({\n",
    "        'SampleID': sample_ids,\n",
    "        'Category': predicted_labels\n",
    "    })\n",
    "\n",
    "    # Step 5: Save the output DataFrame to a new CSV file\n",
    "    output_data.to_csv(output_csv, index=False)\n",
    "    print(f\"Predictions saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m330/330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step\n",
      "Predictions saved to F:/MY_Projects/Deep_Learning_project/data/TextCNN_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "input_csv = \"F:/MY_Projects/Deep_Learning_project/data/test.csv\"  \n",
    "output_csv = \"F:/MY_Projects/Deep_Learning_project/data/TextCNN_predictions.csv\"\n",
    "\n",
    "predict_and_save_as_numbers(model, tokenizer, input_csv, output_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
